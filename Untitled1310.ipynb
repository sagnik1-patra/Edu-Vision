{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35dee4b6-78cd-4c91-afb4-3de5b1abd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# EduVision — Lecture Summarizer\n",
    "# Input: YouTube URL OR local media file (.mp3/.mp4/.wav/...) OR a transcript (.txt/.vtt/.srt)\n",
    "# Output: transcript + short/medium/long summaries + bullet notes + key phrases (saved to disk)\n",
    "# ================================================================\n",
    "import os, re, json, math, tempfile, subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ----------------- USER SETTINGS -----------------\n",
    "# You can set these directly if you run in a notebook.\n",
    "INPUT_SOURCE = \"\"  # e.g., \"https://www.youtube.com/watch?v=...\" OR r\"C:\\path\\to\\lecture.mp4\" OR r\"C:\\path\\to\\transcript.txt\"\n",
    "OUT_DIR = Path(r\"C:\\Users\\sagni\\Downloads\\Edu Vision\\outputs\")\n",
    "ASR_MODEL = \"base\"        # whisper size: tiny | base | small | medium | large\n",
    "DEVICE = \"cuda\" if False else \"cpu\"  # set True above if you have GPU configured (torch.cuda.is_available())\n",
    "SUMMARY_MODEL = \"sshleifer/distilbart-cnn-12-6\"  # small, fast; use \"facebook/bart-large-cnn\" for higher quality\n",
    "\n",
    "# Summary lengths (approximate)\n",
    "SHORT_MAX_WORDS  = 120\n",
    "MEDIUM_MAX_WORDS = 250\n",
    "LONG_MAX_WORDS   = 500\n",
    "\n",
    "# Chunking config for long transcripts (approx)\n",
    "CHUNK_WORDS   = 1200\n",
    "CHUNK_OVERLAP = 150\n",
    "\n",
    "# ----------------- DEPENDENCIES ------------------\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Optional: If you plan to parse .vtt or .srt\n",
    "VTT_SRT_SUPPORT = True\n",
    "\n",
    "# ----------------- UTILITIES ---------------------\n",
    "MEDIA_EXTS = {\".mp3\",\".mp4\",\".m4a\",\".wav\",\".flac\",\".aac\",\".mov\",\".mkv\",\".webm\",\".ogg\"}\n",
    "TEXT_EXTS  = {\".txt\",\".json\",\".vtt\",\".srt\"}\n",
    "\n",
    "def ensure_outdir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def is_url(s: str) -> bool:\n",
    "    return s.lower().startswith((\"http://\",\"https://\",\"www.\"))\n",
    "\n",
    "def download_youtube_audio(url: str) -> Path:\n",
    "    \"\"\"\n",
    "    Download audio from YouTube as a .mp3 using yt-dlp.\n",
    "    Returns local file path.\n",
    "    \"\"\"\n",
    "    ensure_outdir(OUT_DIR)\n",
    "    out_tmpl = str(OUT_DIR / \"yt_audio.%(ext)s\")\n",
    "    cmd = [\n",
    "        \"yt-dlp\",\n",
    "        \"-x\", \"--audio-format\", \"mp3\",\n",
    "        \"-o\", out_tmpl,\n",
    "        url\n",
    "    ]\n",
    "    print(\"[INFO] Downloading audio with yt-dlp...\")\n",
    "    subprocess.run(cmd, check=True)\n",
    "    # Find the produced file\n",
    "    for p in OUT_DIR.glob(\"yt_audio.*\"):\n",
    "        return p\n",
    "    raise RuntimeError(\"yt-dlp did not produce an audio file.\")\n",
    "\n",
    "def read_text_file(path: Path) -> str:\n",
    "    if path.suffix.lower() == \".json\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # If it's a plain dict or list of lines; try to join strings\n",
    "        if isinstance(data, dict) and \"text\" in data:\n",
    "            return str(data[\"text\"])\n",
    "        if isinstance(data, list):\n",
    "            return \"\\n\".join([str(x) for x in data])\n",
    "        return json.dumps(data, ensure_ascii=False, indent=2)\n",
    "    elif path.suffix.lower() in {\".vtt\",\".srt\"} and VTT_SRT_SUPPORT:\n",
    "        return parse_captions_to_text(path)\n",
    "    else:\n",
    "        return Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def parse_captions_to_text(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Very light VTT/SRT to plain text (drop timestamps).\n",
    "    \"\"\"\n",
    "    raw = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    # remove WEBVTT header, SRT indices, timestamps\n",
    "    raw = re.sub(r\"WEBVTT.*?\\n\", \"\", raw, flags=re.IGNORECASE|re.DOTALL)\n",
    "    raw = re.sub(r\"^\\d+\\s*$\", \"\", raw, flags=re.MULTILINE)  # srt cue index lines\n",
    "    raw = re.sub(r\"\\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\s*-->\\s*\\d{2}:\\d{2}:\\d{2}\\.\\d{3}.*\", \"\", raw)\n",
    "    raw = re.sub(r\"\\d{2}:\\d{2}:\\d{2},\\d{3}\\s*-->\\s*\\d{2}:\\d{2}:\\d{2},\\d{3}.*\", \"\", raw)\n",
    "    # collapse multiple newlines\n",
    "    raw = re.sub(r\"\\n{2,}\", \"\\n\", raw)\n",
    "    return raw.strip()\n",
    "\n",
    "def whisper_transcribe(audio_path: Path, model_size=ASR_MODEL, device=DEVICE) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe using openai-whisper (local inference).\n",
    "    \"\"\"\n",
    "    import whisper\n",
    "    print(f\"[INFO] Loading Whisper model: {model_size} on {device}\")\n",
    "    model = whisper.load_model(model_size, device=device)\n",
    "    print(\"[INFO] Transcribing… (this can take a while)\")\n",
    "    result = model.transcribe(str(audio_path))\n",
    "    txt = result.get(\"text\", \"\").strip()\n",
    "    return txt\n",
    "\n",
    "def chunk_text_words(text: str, chunk_words=CHUNK_WORDS, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    words = re.findall(r\"\\S+\", text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        j = min(i + chunk_words, len(words))\n",
    "        chunk = \" \".join(words[i:j])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "        i = j - overlap\n",
    "        if i <= 0: i = j  # first iteration safeguard\n",
    "        if i >= len(words): break\n",
    "    return chunks\n",
    "\n",
    "def load_summarizer(model_name=SUMMARY_MODEL, device=DEVICE):\n",
    "    print(f\"[INFO] Loading summarizer: {model_name}\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    m   = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        m = m.to(\"cuda\")\n",
    "    return tok, m\n",
    "\n",
    "def summarize_chunk(tok, model, text: str, max_words=150) -> str:\n",
    "    # Convert word target to tokens approx\n",
    "    # (rough heuristic: ~1.3 tokens/word for English; adjust as needed)\n",
    "    max_new_tokens = max(64, int(max_words * 1.3))\n",
    "    inputs = tok([text], truncation=True, padding=True, return_tensors=\"pt\", max_length=1024)\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens,\n",
    "            num_beams=4, length_penalty=2.0, early_stopping=True\n",
    "        )\n",
    "    summary = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return summary.strip()\n",
    "\n",
    "def map_reduce_summarize(full_text: str, tok, model, target_words=200) -> str:\n",
    "    # Map: summarize chunks\n",
    "    chunks = chunk_text_words(full_text, CHUNK_WORDS, CHUNK_OVERLAP)\n",
    "    if not chunks:\n",
    "        return summarize_chunk(tok, model, full_text, target_words)\n",
    "    partials = [summarize_chunk(tok, model, ck, max_words=target_words//2) for ck in chunks]\n",
    "    # Reduce: summarize the concatenation of partial summaries\n",
    "    joined = \" \".join(partials)\n",
    "    return summarize_chunk(tok, model, joined, max_words=target_words)\n",
    "\n",
    "def bulletize(text: str, max_bullets=12) -> List[str]:\n",
    "    \"\"\"Very light bullet extraction: split by sentences and pick salient ones by length & keywords.\"\"\"\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # simple scoring: prefer sentences with key edu-ish words or medium length\n",
    "    KEYS = {\"key\",\"main\",\"important\",\"note\",\"definition\",\"example\",\"conclusion\",\"therefore\",\"because\",\"causes\",\"result\",\"summary\"}\n",
    "    scored = []\n",
    "    for s in sents:\n",
    "        t = s.strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        score = 0\n",
    "        w = len(t.split())\n",
    "        score += -abs(w - 18)  # prefer ~18 words\n",
    "        if any(k in t.lower() for k in KEYS):\n",
    "            score += 3\n",
    "        scored.append((score, t))\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [t for _, t in scored[:max_bullets]]\n",
    "\n",
    "def top_key_phrases(text: str, top_k=20) -> List[str]:\n",
    "    \"\"\"Quick key-phrase mining: collocations of 1–3 words, filtered stopwords.\"\"\"\n",
    "    stop = set(\"\"\"\n",
    "        a an the and or if in on with by for to of from that this these those as is are was were be been being have has had do does did not no yes it its it's\n",
    "        at into over under between within without through about across up down out off your you we they he she them his her our their than then there here\n",
    "    \"\"\".split())\n",
    "    tokens = [re.sub(r\"[^a-z0-9\\-]\", \"\", w.lower()) for w in re.findall(r\"\\b[\\w\\-']+\\b\", text)]\n",
    "    tokens = [t for t in tokens if t and t not in stop and not t.isdigit() and len(t) > 2]\n",
    "    # build unigrams/bigrams/trigrams\n",
    "    from collections import Counter\n",
    "    unis = Counter(tokens)\n",
    "    bigs = Counter([\" \".join(tokens[i:i+2]) for i in range(len(tokens)-1)])\n",
    "    tris = Counter([\" \".join(tokens[i:i+3]) for i in range(len(tokens)-2)])\n",
    "    # blend (favor longer phrases)\n",
    "    scores = {}\n",
    "    for k,v in unis.items(): scores[k] = scores.get(k,0) + v\n",
    "    for k,v in bigs.items(): scores[k] = scores.get(k,0) + v*2\n",
    "    for k,v in tris.items(): scores[k] = scores.get(k,0) + v*3\n",
    "    # sort and filter duplicates where longer phrase contains shorter one\n",
    "    phrases = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    final = []\n",
    "    used = set()\n",
    "    for p,_ in phrases:\n",
    "        if any(p in u for u in used if p != u):\n",
    "            continue\n",
    "        used.add(p)\n",
    "        final.append(p)\n",
    "        if len(final) >= top_k:\n",
    "            break\n",
    "    return final\n",
    "\n",
    "def words_count(s: str) -> int:\n",
    "    return len(re.findall(r\"\\S+\", s))\n",
    "\n",
    "# ----------------- MAIN FLOW ---------------------\n",
    "def main(input_source: str):\n",
    "    ensure_outdir(OUT_DIR)\n",
    "\n",
    "    # 1) Resolve input\n",
    "    src = input_source.strip().strip('\"')\n",
    "    if not src:\n",
    "        raise SystemExit(\"Please set INPUT_SOURCE to a YouTube URL, a local media path, or a transcript file.\")\n",
    "\n",
    "    if is_url(src):\n",
    "        audio_path = download_youtube_audio(src)\n",
    "        transcript = whisper_transcribe(audio_path)\n",
    "        basename = \"youtube_lecture\"\n",
    "    else:\n",
    "        p = Path(src)\n",
    "        if not p.exists():\n",
    "            raise SystemExit(f\"Input not found: {p}\")\n",
    "        if p.suffix.lower() in MEDIA_EXTS:\n",
    "            transcript = whisper_transcribe(p)\n",
    "            basename = p.stem\n",
    "        elif p.suffix.lower() in TEXT_EXTS:\n",
    "            transcript = read_text_file(p)\n",
    "            basename = p.stem\n",
    "        else:\n",
    "            # try anyway: whisper handles many formats if ffmpeg can read it\n",
    "            try:\n",
    "                transcript = whisper_transcribe(p)\n",
    "                basename = p.stem\n",
    "            except Exception as e:\n",
    "                raise SystemExit(f\"Unsupported input type and ASR failed: {e}\")\n",
    "\n",
    "    # Save transcript\n",
    "    transcript_path = OUT_DIR / f\"{basename}_transcript.txt\"\n",
    "    transcript_path.write_text(transcript, encoding=\"utf-8\")\n",
    "    print(f\"[SAVE] Transcript -> {transcript_path} ({words_count(transcript)} words)\")\n",
    "\n",
    "    # 2) Load summarizer\n",
    "    tok, model = load_summarizer(SUMMARY_MODEL, DEVICE)\n",
    "\n",
    "    # 3) Summaries (short/medium/long)\n",
    "    print(\"[INFO] Summarizing (short)…\")\n",
    "    summary_short  = map_reduce_summarize(transcript, tok, model, target_words=SHORT_MAX_WORDS)\n",
    "    print(\"[INFO] Summarizing (medium)…\")\n",
    "    summary_medium = map_reduce_summarize(transcript, tok, model, target_words=MEDIUM_MAX_WORDS)\n",
    "    print(\"[INFO] Summarizing (long)…\")\n",
    "    summary_long   = map_reduce_summarize(transcript, tok, model, target_words=LONG_MAX_WORDS)\n",
    "\n",
    "    # 4) Notes & key phrases\n",
    "    bullets    = bulletize(summary_long, max_bullets=14)\n",
    "    keyphrases = top_key_phrases(transcript, top_k=25)\n",
    "\n",
    "    # 5) Save all artifacts\n",
    "    (OUT_DIR / f\"{basename}_summary_short.txt\").write_text(summary_short,  encoding=\"utf-8\")\n",
    "    (OUT_DIR / f\"{basename}_summary_medium.txt\").write_text(summary_medium, encoding=\"utf-8\")\n",
    "    (OUT_DIR / f\"{basename}_summary_long.txt\").write_text(summary_long,   encoding=\"utf-8\")\n",
    "\n",
    "    notes_md = \"# Bullet Notes\\n\\n\" + \"\\n\".join([f\"- {b}\" for b in bullets])\n",
    "    (OUT_DIR / f\"{basename}_notes.md\").write_text(notes_md, encoding=\"utf-8\")\n",
    "\n",
    "    meta = {\n",
    "        \"input\": src,\n",
    "        \"asr_model\": ASR_MODEL,\n",
    "        \"summary_model\": SUMMARY_MODEL,\n",
    "        \"words_transcript\": words_count(transcript),\n",
    "        \"chunks_config\": {\"chunk_words\": CHUNK_WORDS, \"overlap\": CHUNK_OVERLAP},\n",
    "        \"lengths\": {\n",
    "            \"short_words\":  SHORT_MAX_WORDS,\n",
    "            \"medium_words\": MEDIUM_MAX_WORDS,\n",
    "            \"long_words\":   LONG_MAX_WORDS\n",
    "        },\n",
    "        \"key_phrases\": keyphrases[:25],\n",
    "    }\n",
    "    (OUT_DIR / f\"{basename}_meta.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n[DONE] Saved:\")\n",
    "    print(\" -\", transcript_path)\n",
    "    print(\" -\", OUT_DIR / f\"{basename}_summary_short.txt\")\n",
    "    print(\" -\", OUT_DIR / f\"{basename}_summary_medium.txt\")\n",
    "    print(\" -\", OUT_DIR / f\"{basename}_summary_long.txt\")\n",
    "    print(\" -\", OUT_DIR / f\"{basename}_notes.md\")\n",
    "    print(\" -\", OUT_DIR / f\"{basename}_meta.json\")\n",
    "\n",
    "# ----------------- ENTRYPOINT --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # If you want CLI usage, uncomment below and run:\n",
    "    # python lecture_summarizer.py --input \"<url or path>\" --outdir \"C:\\Users\\sagni\\Downloads\\Edu Vision\\outputs\"\n",
    "    #\n",
    "    # import argparse\n",
    "    # ap = argparse.ArgumentParser()\n",
    "    # ap.add_argument(\"--input\", required=True, help=\"YouTube URL, media file, or transcript file\")\n",
    "    # ap.add_argument(\"--outdir\", default=str(OUT_DIR), help=\"Output directory\")\n",
    "    # ap.add_argument(\"--asr_model\", default=ASR_MODEL, help=\"Whisper size: tiny/base/small/medium/large\")\n",
    "    # ap.add_argument(\"--device\", default=DEVICE, help=\"cpu or cuda\")\n",
    "    # ap.add_argument(\"--summary_model\", default=SUMMARY_MODEL)\n",
    "    # args = ap.parse_args()\n",
    "    #\n",
    "    # INPUT_SOURCE = args.input\n",
    "    # OUT_DIR = Path(args.outdir); ensure_outdir(OUT_DIR)\n",
    "    # ASR_MODEL = args.asr_model; DEVICE = args.device; SUMMARY_MODEL = args.summary_model\n",
    "    pass\n",
    "\n",
    "# To run in a notebook or directly:\n",
    "# 1) Set INPUT_SOURCE above, e.g.:\n",
    "#    INPUT_SOURCE = \"https://www.youtube.com/watch?v=XXXXX\"\n",
    "#    or INPUT_SOURCE = r\"C:\\path\\to\\lecture.mp4\"\n",
    "#    or INPUT_SOURCE = r\"C:\\path\\to\\transcript.txt\"\n",
    "# 2) Then call:\n",
    "# main(INPUT_SOURCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e51ea3-6404-4f9d-aab5-afabdcd4e051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
